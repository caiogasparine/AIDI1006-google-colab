{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrg7qdDCf7gi9tWdpQRVie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiogasparine/AIDI1006-google-colab/blob/main/OpenAI_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0aUhr310GFn",
        "outputId": "425c04d9-bdb2-4236-8d44-a8b5cdc95ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.0\n"
          ]
        }
      ],
      "source": [
        "# nano ~/.bashrc\n",
        "# export OPENAI_API_KEY='XXXX'\n",
        "# run commands: source ~/.bashrc // source ~/.bashprofile\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To use environment variable please add your OPENAI_API_KEY as an environment variable and confirm it using the\n",
        "# code below - to confirm the content of the variable using os.environment\n",
        "# import os\n",
        "# print(os.environ['OPENAI_API_KEY'])"
      ],
      "metadata": {
        "id": "fae546Q80WE-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import time                   # for measuring time duration of API calls\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"sk-proj-\"))\n",
        "GPT_MODEL = 'gpt-3.5-turbo'"
      ],
      "metadata": {
        "id": "mpS7cXEPcLcp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of Canada? When was it defined as capital?\"\n",
        "#prompt = \"What is the name of the Canadian Prime Minister in 2021?\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Your response should be in JSON format.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")\n",
        "print(\"RAW RESPONSE - CHAT COMPLETION OBJECT\\n\",response)\n",
        "print(\"--\"*10)\n",
        "print(\"ONLY RESPONSE\\n\",response.choices[0].message.content)\n",
        "print(\"--\"*10)\n",
        "data = json.loads(response.model_dump_json())\n",
        "json_str = json.dumps(data, indent=3)\n",
        "print(\"JSON FORMAT\\n\", json_str)"
      ],
      "metadata": {
        "id": "8X_oSzj3cPI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52087d14-0711-4e13-849d-e2252df4ed92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW RESPONSE - CHAT COMPLETION OBJECT\n",
            " ChatCompletion(id='chatcmpl-9oeLMLBkks97GIxesIlcCBeby2GdE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"capital\": \"Ottawa\",\\n  \"defined_as_capital_year\": 1857\\n}', role='assistant', function_call=None, tool_calls=None))], created=1721858508, model='gpt-4-1106-preview', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=33, total_tokens=56))\n",
            "--------------------\n",
            "ONLY RESPONSE\n",
            " {\n",
            "  \"capital\": \"Ottawa\",\n",
            "  \"defined_as_capital_year\": 1857\n",
            "}\n",
            "--------------------\n",
            "JSON FORMAT\n",
            " {\n",
            "   \"id\": \"chatcmpl-9oeLMLBkks97GIxesIlcCBeby2GdE\",\n",
            "   \"choices\": [\n",
            "      {\n",
            "         \"finish_reason\": \"stop\",\n",
            "         \"index\": 0,\n",
            "         \"logprobs\": null,\n",
            "         \"message\": {\n",
            "            \"content\": \"{\\n  \\\"capital\\\": \\\"Ottawa\\\",\\n  \\\"defined_as_capital_year\\\": 1857\\n}\",\n",
            "            \"role\": \"assistant\",\n",
            "            \"function_call\": null,\n",
            "            \"tool_calls\": null\n",
            "         }\n",
            "      }\n",
            "   ],\n",
            "   \"created\": 1721858508,\n",
            "   \"model\": \"gpt-4-1106-preview\",\n",
            "   \"object\": \"chat.completion\",\n",
            "   \"service_tier\": null,\n",
            "   \"system_fingerprint\": null,\n",
            "   \"usage\": {\n",
            "      \"completion_tokens\": 23,\n",
            "      \"prompt_tokens\": 33,\n",
            "      \"total_tokens\": 56\n",
            "   }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an OpenAI ChatCompletion request\n",
        "# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
        "\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a ChatCompletion request to count to 100\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "# calculate the time it took to receive the response\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(\"--\"*10)\n",
        "print(f\"Full response received:\\n{response}\")\n",
        "print(\"--\"*10)\n",
        "data = json.loads(response.model_dump_json())\n",
        "json_str = json.dumps(data, indent=3)\n",
        "print(\"JSON FORMAT\\n\", json_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izuy9VCEL6PC",
        "outputId": "46dd2709-bf91-4949-e6cc-8af7f42e0d69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full response received 4.88 seconds after request\n",
            "--------------------\n",
            "Full response received:\n",
            "ChatCompletion(id='chatcmpl-9oeJt8fvm7TOodSIgFZXS50dWYSrf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100', role='assistant', function_call=None, tool_calls=None))], created=1721858417, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=298, prompt_tokens=36, total_tokens=334))\n",
            "--------------------\n",
            "JSON FORMAT\n",
            " {\n",
            "   \"id\": \"chatcmpl-9oeJt8fvm7TOodSIgFZXS50dWYSrf\",\n",
            "   \"choices\": [\n",
            "      {\n",
            "         \"finish_reason\": \"stop\",\n",
            "         \"index\": 0,\n",
            "         \"logprobs\": null,\n",
            "         \"message\": {\n",
            "            \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\",\n",
            "            \"role\": \"assistant\",\n",
            "            \"function_call\": null,\n",
            "            \"tool_calls\": null\n",
            "         }\n",
            "      }\n",
            "   ],\n",
            "   \"created\": 1721858417,\n",
            "   \"model\": \"gpt-3.5-turbo-0125\",\n",
            "   \"object\": \"chat.completion\",\n",
            "   \"service_tier\": null,\n",
            "   \"system_fingerprint\": null,\n",
            "   \"usage\": {\n",
            "      \"completion_tokens\": 298,\n",
            "      \"prompt_tokens\": 36,\n",
            "      \"total_tokens\": 334\n",
            "   }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an OpenAI ChatCompletion request\n",
        "# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
        "\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a ChatCompletion request to count to 100\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Your response should be in JSON format.\"},\n",
        "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "# calculate the time it took to receive the response\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(\"--\"*10)\n",
        "print(f\"Full response received:\\n{response}\")\n",
        "print(\"--\"*10)\n",
        "data = json.loads(response.model_dump_json())\n",
        "json_str = json.dumps(data, indent=3)\n",
        "print(\"JSON FORMAT\\n\", json_str)"
      ],
      "metadata": {
        "id": "5IjsaPKUcTBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973df298-9898-4c3a-ec8d-668ba95f9d2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full response received 14.55 seconds after request\n",
            "--------------------\n",
            "Full response received:\n",
            "ChatCompletion(id='chatcmpl-9oeJyRNPVDpUF0Do3hMQ7fXT8Ytnb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\\n{\\n  \"sequence\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\"\\n}\\n```', role='assistant', function_call=None, tool_calls=None))], created=1721858422, model='gpt-4-1106-preview', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=310, prompt_tokens=48, total_tokens=358))\n",
            "--------------------\n",
            "JSON FORMAT\n",
            " {\n",
            "   \"id\": \"chatcmpl-9oeJyRNPVDpUF0Do3hMQ7fXT8Ytnb\",\n",
            "   \"choices\": [\n",
            "      {\n",
            "         \"finish_reason\": \"stop\",\n",
            "         \"index\": 0,\n",
            "         \"logprobs\": null,\n",
            "         \"message\": {\n",
            "            \"content\": \"```json\\n{\\n  \\\"sequence\\\": \\\"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\\\"\\n}\\n```\",\n",
            "            \"role\": \"assistant\",\n",
            "            \"function_call\": null,\n",
            "            \"tool_calls\": null\n",
            "         }\n",
            "      }\n",
            "   ],\n",
            "   \"created\": 1721858422,\n",
            "   \"model\": \"gpt-4-1106-preview\",\n",
            "   \"object\": \"chat.completion\",\n",
            "   \"service_tier\": null,\n",
            "   \"system_fingerprint\": null,\n",
            "   \"usage\": {\n",
            "      \"completion_tokens\": 310,\n",
            "      \"prompt_tokens\": 48,\n",
            "      \"total_tokens\": 358\n",
            "   }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an OpenAI ChatCompletion request with stream=True\n",
        "# https://platform.openai.com/docs/api-reference/streaming#chat/create-stream\n",
        "\n",
        "# a ChatCompletion request\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    stream=True  # this time, we set stream=True\n",
        ")\n",
        "print(response)\n",
        "\n",
        "for chunk in response:\n",
        "    #print(chunk)\n",
        "    print(chunk.choices[0].delta.content)\n",
        "    print(\"****************\")"
      ],
      "metadata": {
        "id": "eIco6ig0cWc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381c4e6b-f1d1-4135-8717-427e0cd919bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<openai.Stream object at 0x7b91c598cf70>\n",
            "\n",
            "****************\n",
            "Two\n",
            "****************\n",
            ".\n",
            "****************\n",
            "None\n",
            "****************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBLEF8BtcaJ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "bad_request = \"I want to hurt them. How can i do this?\"\n",
        "good_request = \"I would kill for a cup of coffe. Where can I get one nearby?\"\n",
        "\n",
        "async def check_moderation_flag(expression):\n",
        "    moderation_response = client.moderations.create(input=expression)\n",
        "    flagged = moderation_response.results[0].flagged\n",
        "    return flagged\n",
        "\n",
        "async def get_chat_response(user_request):\n",
        "    print(\"Getting LLM response\")\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_request},\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL, messages=messages, temperature=0.5\n",
        "    )\n",
        "    print(\"Got LLM response\")\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "async def execute_chat_with_input_moderation(user_request):\n",
        "    # Create tasks for moderation and chat response\n",
        "    moderation_task = asyncio.create_task(check_moderation_flag(user_request))\n",
        "    chat_task = asyncio.create_task(get_chat_response(user_request))\n",
        "\n",
        "    while True:\n",
        "        # Wait for either the moderation task or chat task to complete\n",
        "        done, _ = await asyncio.wait(\n",
        "            [moderation_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n",
        "        )\n",
        "\n",
        "        # If moderation task is not completed, wait and continue to the next iteration\n",
        "        if moderation_task not in done:\n",
        "            await asyncio.sleep(0.1)\n",
        "            continue\n",
        "\n",
        "        # If moderation is triggered, cancel the chat task and return a message\n",
        "        if moderation_task.result() == True:\n",
        "            chat_task.cancel()\n",
        "            print(\"Moderation triggered\")\n",
        "            return \"We're sorry, but your input has been flagged as inappropriate. Please rephrase your input and try again.\"\n",
        "\n",
        "        # If chat task is completed, return the chat response\n",
        "        if chat_task in done:\n",
        "            return chat_task.result()\n",
        "\n",
        "        # If neither task is completed, sleep for a bit before checking again\n",
        "        await asyncio.sleep(0.1)"
      ],
      "metadata": {
        "id": "WFc7avCNcd2Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the main function with the good request - this should go through\n",
        "good_response = await execute_chat_with_input_moderation(good_request)\n",
        "print(good_response)"
      ],
      "metadata": {
        "id": "OqxNPwPLcgd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e935b0-0341-4dcd-c7c5-89abd5cf9e2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting LLM response\n",
            "Got LLM response\n",
            "I'm glad to help! To find a nearby coffee shop, you can use a search engine like Google Maps or a food delivery app like Uber Eats or DoorDash. Just type in \"coffee shop\" or \"cafe\" in the search bar and it will show you a list of options in your area. You can also ask locals or check out popular coffee chains like Starbucks or Dunkin' Donuts that are usually found in many locations. Enjoy your cup of coffee!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the main function with the bad request - this should get blocked\n",
        "bad_response = await execute_chat_with_input_moderation(bad_request)\n",
        "print(bad_response)"
      ],
      "metadata": {
        "id": "YjVj6TWdqK-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d74e84-dc2d-4cd1-b3ac-b4e085a0d17b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting LLM response\n",
            "Got LLM response\n",
            "Moderation triggered\n",
            "We're sorry, but your input has been flagged as inappropriate. Please rephrase your input and try again.\n"
          ]
        }
      ]
    }
  ]
}